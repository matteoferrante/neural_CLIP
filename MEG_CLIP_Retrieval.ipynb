{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceaf254-5094-4527-bcdf-4ffe73b54d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne_bids import (\n",
    "    BIDSPath,\n",
    "    read_raw_bids,\n",
    "    print_dir_tree,\n",
    "    make_report,\n",
    "    find_matching_paths,\n",
    "    get_entity_vals,\n",
    ")\n",
    "\n",
    "import h5py\n",
    "from os.path import join as opj\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import tqdm\n",
    "from versatile_diffusion_dual_guided_fake_images import *\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e97a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_dir=\"/home/matteo/data/THINGS_MEG/LOCAL/ocontier/thingsmri/openneuro/THINGS-data/THINGS-MEG/ds004212/derivatives/preprocessed/\"\n",
    "p=1\n",
    "epochs = mne.read_epochs(f'{preproc_dir}/preprocessed_P{str(p)}-epo.fif', preload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896438d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=epochs[epochs.metadata[\"trial_type\"] == \"exp\"]\n",
    "test_data=epochs[epochs.metadata[\"trial_type\"] == \"test\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dacd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to load images\n",
    "\n",
    "stimuli_path=opj(\"/home/matteo/data/THINGS_img/THINGS\",\"Images\")\n",
    "\n",
    "img_class=train_data.metadata.image_path.iloc[0].split(\"images_meg/\")[-1].split(\"/\")[0]\n",
    "img_name=train_data.metadata.image_path.iloc[0].split(\"images_meg/\")[-1].split(\"/\")[1]\n",
    "\n",
    "Image.open(opj(stimuli_path,img_class,img_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a73a88",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train=[opj(stimuli_path,img_class,img_name) for img_class,img_name in zip(train_data.metadata.image_path.str.split(\"images_meg/\").str[0],train_data.metadata.image_path.str.split(\"images_meg/\").str[1])]\n",
    "# img_test=[opj(stimuli_path,img_class,img_name) for img_class,img_name in zip(test_data.metadata.image_path.str.split(\"images_test_meg/\").str[0],test_data.metadata.image_path.str.split(\"images_meg/\").str[1])]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc5c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.metadata.image_path.iloc[0].split(\"images_test_meg/\")[-1]\n",
    "\n",
    "img_test=[]\n",
    "for i in range(len(test_data.metadata)):\n",
    "    filename=test_data.metadata.image_path.iloc[i].split(\"images_test_meg/\")[-1]\n",
    "    img_class=filename[:filename.rfind(\"_\")]\n",
    "    img_test.append(opj(stimuli_path,img_class,filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b71d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=64\n",
    "device=\"cuda:0\"\n",
    "\n",
    "pipe_embed = VersatileDiffusionDualGuidedFromCLIPEmbeddingPipeline.from_pretrained(\"shi-labs/versatile-diffusion\", )\n",
    "\n",
    "pipe_embed.remove_unused_weights()\n",
    "pipe_embed = pipe_embed.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172f64a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clip_img_embeds=[]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm.tqdm(range(0,len(img_train),batch)):\n",
    "\n",
    "        #save img data\n",
    "        y= img_train[i:i+batch]\n",
    "                         \n",
    "        images=[Image.open(i).convert(\"RGB\") for i in y]\n",
    "\n",
    "        #encode images in CLIP\n",
    "        image_features=pipe_embed._encode_image_prompt(images,device=device,num_images_per_prompt=1,do_classifier_free_guidance=False).cpu()\n",
    "        train_clip_img_embeds.append(image_features)\n",
    "\n",
    "    \n",
    "        \n",
    "    train_clip_img_embeds = torch.cat(train_clip_img_embeds,axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e4756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_clip_img_embeds=[]\n",
    "with torch.no_grad():\n",
    "    for i in tqdm.tqdm(range(0,len(img_test),batch)):\n",
    "\n",
    "        #save img data\n",
    "        y= img_test[i:i+batch]\n",
    "                         \n",
    "        images=[Image.open(i).convert(\"RGB\") for i in y]\n",
    "\n",
    "        #encode images in CLIP\n",
    "        image_features=pipe_embed._encode_image_prompt(images,device=device,num_images_per_prompt=1,do_classifier_free_guidance=False).cpu()\n",
    "        test_clip_img_embeds.append(image_features)\n",
    "\n",
    "    \n",
    "        \n",
    "    test_clip_img_embeds = torch.cat(test_clip_img_embeds,axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05293bc",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_data.get_data()).float(),train_clip_img_embeds[:,0])\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_data.get_data()).float(),test_clip_img_embeds[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=train_dataset[0]    \n",
    "\n",
    "\n",
    "\n",
    "BS=256\n",
    "\n",
    "clip_train_dataloader=DataLoader(train_dataset, batch_size=BS, shuffle=True)\n",
    "clip_test_dataloader=DataLoader(test_dataset, batch_size=BS, shuffle=False)\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9d50fe",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b80e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(num_input_channels, c_hid, kernel_size=11, padding=1, stride=3),  # 32 => 16\n",
    "            act_fn(),\n",
    "            nn.Conv1d(c_hid, c_hid, kernel_size=7, padding=1,stride=2),\n",
    "            act_fn(),\n",
    "            nn.Conv1d(c_hid, 2 * c_hid, kernel_size=5, padding=1, stride=2),  # 16 => 8\n",
    "            act_fn(),\n",
    "            nn.Conv1d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1,stride=2),\n",
    "            act_fn(),\n",
    "            nn.Conv1d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 8 => 4\n",
    "            act_fn(),\n",
    "            nn.Conv1d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 4 => 2\n",
    "            act_fn(),\n",
    "            nn.Flatten(),  # Sequence to single feature vector\n",
    "            nn.LazyLinear(latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bfb3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class ContrastiveModel(pl.LightningModule):\n",
    "\n",
    "    def contrastive_loss(self, z_i, z_j):\n",
    "\n",
    "        z_i = nn.functional.normalize(z_i, dim=1)\n",
    "        z_j = nn.functional.normalize(z_j, dim=1)\n",
    "        \n",
    "        logits = (z_i @ z_j.T) / self.temperature\n",
    "        similarities = z_j @ z_j.T\n",
    "        # targets = torch.nn.functional.softmax(similarities * self.temperature, dim=-1)\n",
    "\n",
    "        targets = torch.arange(logits.shape[0]).long().to(logits.device)\n",
    "        \n",
    "        loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        return loss\n",
    "    \n",
    "    def mean_contrastive(self, z_i, z_j, temperature=1.0):\n",
    "        return nn.functional.mse_loss(z_i, z_j)+self.contrastive_loss(z_i, z_j, temperature=temperature)/8\n",
    "    \n",
    "    def cosine_loss(self, z_i, z_j, temperature=1.0):\n",
    "        cosine_similarity = torch.nn.functional.cosine_similarity(z_i, z_j).mean()\n",
    "        return 1- cosine_similarity\n",
    "\n",
    "    def __init__(self,  num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU, temperature=.1,loss_type=\"contrastive\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.temperature=temperature\n",
    "        \n",
    "        self.model = Encoder(num_input_channels, base_channel_size, latent_dim, act_fn)\n",
    "\n",
    "        self.loss_type=loss_type\n",
    "        if loss_type==\"contrastive\":\n",
    "            self.loss_fn=self.contrastive_loss\n",
    "        elif loss_type==\"mean_contrastive\":\n",
    "            self.loss_fn=self.mean_contrastive\n",
    "        \n",
    "        elif loss_type==\"mse\":\n",
    "            self.loss_fn=torch.nn.functional.mse_loss\n",
    "        elif loss_type==\"cosine\":\n",
    "            self.loss_fn=self.cosine_loss\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.train_mse=[]\n",
    "        self.train_cosine=[]\n",
    "        self.val_losses = []\n",
    "        self.val_mse=[]\n",
    "        self.val_cosine=[]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y= batch\n",
    "        # x = x.float()\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "\n",
    "        mse_loss = torch.nn.functional.mse_loss(y_hat, y)\n",
    "        cosine_similarity = torch.nn.functional.cosine_similarity(y_hat, y).mean()\n",
    "        self.train_mse.append(mse_loss.item())\n",
    "        self.train_cosine.append(cosine_similarity.item())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # x = x.float()\n",
    "\n",
    "        y_hat = self(x)\n",
    "\n",
    "        loss=self.loss_fn(y_hat, y)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        mse_loss = torch.nn.functional.mse_loss(y_hat, y)\n",
    "        self.log('val_mse_loss', mse_loss, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        cosine_similarity = torch.nn.functional.cosine_similarity(y_hat, y).mean()\n",
    "        self.log('val_cosine_similarity', cosine_similarity, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        self.val_losses.append(loss.item())\n",
    "        self.val_mse.append(mse_loss.item())\n",
    "        self.val_cosine.append(cosine_similarity.item())\n",
    "        return mse_loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        # return torch.optim.AdamW(self.parameters(), lr=3e-4, weight_decay=0)\n",
    "        # add a scheduler\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "        # use a scheduler that every 100 steps, it will reduce the learning rate by 0.1\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=50, verbose=True)\n",
    "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00edf649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d532a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_model = ContrastiveModel(num_input_channels= 271, base_channel_size=192, latent_dim=768,act_fn=nn.ReLU, loss_type=\"contrastive\")\n",
    "\n",
    "summary(brain_model, x.shape,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27d0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Instantiate a PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(max_epochs=30, devices=[0])\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(brain_model, clip_train_dataloader, clip_test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ed4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(brain_model.train_mse, label='train')\n",
    "plt.plot(brain_model.val_mse, label='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e0254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(brain_model.train_cosine, label='train')\n",
    "plt.plot(brain_model.val_cosine, label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=next(iter(clip_test_dataloader))\n",
    "\n",
    "y_pred=[]\n",
    "with torch.no_grad():\n",
    "\n",
    "    for x,y in tqdm.tqdm(clip_test_dataloader):\n",
    "        y_hat=brain_model(x).cpu()\n",
    "        y_pred.append(y_hat)\n",
    "\n",
    "y_pred=torch.cat(y_pred,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b92504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity=  torch.nn.functional.softmax(.1*torch.nn.functional.normalize(test_clip_img_embeds[:,0],-1) @ torch.nn.functional.normalize(y_pred,.1).T)\n",
    "\n",
    "z_i=y_pred\n",
    "z_j=test_clip_img_embeds[:,0]\n",
    "\n",
    "z_i = nn.functional.normalize(z_i, dim=1)\n",
    "z_j = nn.functional.normalize(z_j, dim=1)\n",
    "\n",
    "logits = (z_i @ z_j.T) / .1\n",
    "\n",
    "similarity =  torch.nn.functional.softmax(logits,-1)\n",
    "top_indices=torch.topk(similarity,50,1).indices\n",
    "# similarity=  torch.nn.functional.softmax(10*torch.nn.functional.normalize(test_clip_img_embeds[:,0],-1) @ torch.nn.functional.normalize(y_pred.T,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c89ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06326bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## voglio selezionare immagini diverse!\n",
    "\n",
    "selected_images=[]\n",
    "for top_idxs in top_indices:\n",
    "    selected_images.append(list(set(np.array(img_test)[top_idxs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bdce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb07f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=50\n",
    "fig, ax = plt.subplots(5,6,figsize=(10,5))  \n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    ax[i,0].imshow(Image.open(img_test[idx+i]).convert(\"RGB\").resize((224,224)))\n",
    "    ax[i,1].imshow(Image.open(selected_images[idx+i][0]).convert(\"RGB\").resize((224,224)))\n",
    "\n",
    "    ax[i,2].imshow(Image.open(selected_images[idx+i][1]).convert(\"RGB\").resize((224,224)))\n",
    "    ax[i,3].imshow(Image.open(selected_images[idx+i][2]).convert(\"RGB\").resize((224,224)))\n",
    "    ax[i,4].imshow(Image.open(selected_images[idx+i][3]).convert(\"RGB\").resize((224,224)))\n",
    "\n",
    "    ax[i,5].imshow(Image.open(selected_images[idx+i][4]).convert(\"RGB\").resize((224,224)))\n",
    "\n",
    "    ax[i,0].axis(\"off\")\n",
    "    ax[i,1].axis(\"off\")\n",
    "    ax[i,2].axis(\"off\")\n",
    "    ax[i,3].axis(\"off\")\n",
    "    ax[i,4].axis(\"off\")\n",
    "    ax[i,5].axis(\"off\")\n",
    "\n",
    "ax[0,0].set_title(\"Original\")   \n",
    "\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08927b80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(f\"models_contrastive_MEG\",exist_ok=True)\n",
    "\n",
    "torch.save(brain_model, \"models_contrastive_MEG/whole_pl_model.pt\")\n",
    "torch.save(brain_model.state_dict(), \"models_contrastive_MEG/whole_pl_model_state_dict.pt\")\n",
    "\n",
    "torch.save(brain_model.model, \"models_contrastive_MEG/encoder.pt\")\n",
    "torch.save(brain_model.model.state_dict(), \"models_contrastive_MEG/encoder_state_dict.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
